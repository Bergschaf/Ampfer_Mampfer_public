{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMirTlCRuFDN392hPsfXzfS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bergschaf/Ampfer_Mampfer_public/blob/main/Ampfer_Mampfer_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOahqX1nQwJK"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "%%bash\n",
        "pip install haversine\n",
        "pip install simplekml\n",
        "pip install exif\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../\n",
        "cd ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Images\n",
        "import os\n",
        "if not os.path.exists(\"Bilder\"):\n",
        "  os.mkdir(\"Bilder\") \n",
        "!gdown --folder https://drive.google.com/drive/folders/1U898P7M925KQ89KvOIjYJAwuzZxjRDtP?usp=sharing -q\n",
        "os.chdir(\"/content\")\n",
        "if not os.path.exists(\"Bilder_1m\"):\n",
        "  os.mkdir(\"Bilder_1m\") \n",
        "!gdown --folder https://drive.google.com/drive/folders/1yqKWkRHdbfD9YGNqSf8z6cUdFYV7vLfE?usp=sharing -q\n",
        "os.chdir(\"/content\")"
      ],
      "metadata": {
        "id": "s-xFxx14mjZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Networks\n",
        "import os\n",
        "if not os.path.exists(\"Networks\"):\n",
        "  os.mkdir(\"Networks\") \n",
        "os.chdir(\"Networks\")\n",
        "!gdown 1k3Yjk6Emb_ePUgzQIfSdJq8jxgRoNpg4\n",
        "!gdown 1cEqA5yUS2CjSEkigx1dpTSRHrMraPBge\n",
        "os.chdir(\"/content\")"
      ],
      "metadata": {
        "id": "PeE6Vk99n8ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ermitteln der Koordinaten der Ampfer\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from engine import evaluate\n",
        "from PIL import Image\n",
        "import utils\n",
        "import transforms as T\n",
        "import os\n",
        "import math\n",
        "import haversine\n",
        "from haversine import inverse_haversine\n",
        "from exif import Image as EImage\n",
        "import json\n",
        "import simplekml\n",
        "\n",
        "\n",
        "if not os.path.exists(\"Koordinaten\"):\n",
        "  os.mkdir(\"Koordinaten\")\n",
        "bilderp = \"Bilder/\"  # Ordner, in den die Bilder hochgeladen wurden\n",
        "targetp = \"Koordinaten/\"  # Ordner, in den die ermittelten Koordinaten gespeichert werden\n",
        "\n",
        "\n",
        "class Dataloader(torch.utils.data.Dataset):\n",
        "    def __init__(self, transforms=None):\n",
        "        self.Bilder = bilderp\n",
        "        self.pdir = sorted(os.listdir(self.Bilder))\n",
        "        self.Data = {}\n",
        "        self.Boxes = []\n",
        "        self.Transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Zum Trainieren des Netzes soll diese Funktion, die Label des Bildes an der Position idx\n",
        "        zurückgeben. Da hier das Netz nur angewendet und nicht trainiert wird, werden diese Label\n",
        "        nicht benötigt. Anstelle der Label wird daher der Dateiname des Bildes zurückgegeben, sodass\n",
        "        man die erkannten Ampfer später den Metadaten des Bildes zuordnen kann.\n",
        "        \"\"\"\n",
        "        img = Image.open(self.Bilder + self.pdir[idx]).convert(\"RGB\")\n",
        "        filename = {\"filename\": self.pdir[idx]}\n",
        "        if self.Transforms is not None:\n",
        "            img, filename = self.Transforms(img, filename)\n",
        "        return img, filename\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.pdir))\n",
        "\n",
        "\n",
        "def get_model(num_classes):\n",
        "    \"\"\"\n",
        "    Diese Funktion ist dafür zuständig, das Netz zu laden.\n",
        "    \"\"\"\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)  # Das pretrained-Model von pytorch\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features  # wird heruntergeladen und konfiguriert.\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    \n",
        "    if os.path.isfile(\"Networks/model_drohne_niedrig_V2.pt\"):\n",
        "        print(\"---loaded model---\")\n",
        "        model.load_state_dict(torch.load(\"Networks/model_drohne_niedrig_V2.pt\"))\n",
        "    else:\n",
        "        print(\"---model not found---\")\n",
        "    # Wenn bereits ein trainiertes Netz vorhanden ist, dann werden die entsprechenden Gewichtungen geladen:\n",
        "    return model\n",
        "\n",
        "\n",
        "class Img:\n",
        "    \"\"\"\n",
        "    Die Img Klasse wird für Bilder erstellt, um die Überlappung der Boxen, die die Ampferpflanzen markieren, zu\n",
        "    entfernen.\n",
        "    \"\"\"\n",
        "    def __init__(self, filename: str, objects):\n",
        "        self.filename = filename  # Dateiname des Bildes (ohne Dateiendung)\n",
        "        self.objects = objects  # Die erkannten Positionen der Ampferpflanzen.\n",
        "\n",
        "    def remove_overlap(self):\n",
        "        \"\"\"\n",
        "        Diese Funktion überprüft, ob sich die Boxen überlappen.\n",
        "        \"\"\"\n",
        "        for index, object in enumerate(self.objects):\n",
        "            for index2, object2 in enumerate(self.objects[index:]):\n",
        "                # Das Programm überorüft für jede Kombination von zwei Boxen, ob sie sich überlappen.\n",
        "                overlap, smaller = check_overlap(object, object2)  # Die check_overlap-Funktion gibt zurück, zu wie groß\n",
        "                # die sich überlappende Fläche im Verlgeich zur Fläche der kleineren Box ist (in Prozent).\n",
        "                if overlap > 0.8:  # Wenn sich die Boxen zu mehr als 80 % überlappen, dann wird die kleinere gelöscht.\n",
        "                    try:\n",
        "                        if smaller == 1:  # Die smaller Variable gibt an, welche Box kleiner ist.\n",
        "                            self.objects.remove(object)\n",
        "                        else:\n",
        "                            self.objects.remove(object2)\n",
        "                    except ValueError:  # Wenn die Box schon gelöscht wurde, dann wird nichts gemacht.\n",
        "                        pass\n",
        "\n",
        "\n",
        "def check_overlap(pos1, pos2):\n",
        "    \"\"\"\n",
        "    Die check_overlap-Funktion gibt zurück, wie viel sich die zwei Boxen pos1 und pos2 überlappen.\n",
        "    \"\"\"\n",
        "    difx = 0  # Die überlappung in x-Richtung.\n",
        "    if pos2[0] < pos1[0] < pos2[2]:\n",
        "        difx = pos2[2] - pos1[0]\n",
        "        if pos2[0] < pos1[2] < pos2[2]:\n",
        "            difx = pos2[2] - pos2[0]\n",
        "\n",
        "    elif pos1[0] < pos2[0] < pos1[2]:\n",
        "        difx = pos1[2] - pos2[0]\n",
        "        if pos1[0] < pos2[2] < pos1[2]:\n",
        "            difx = pos1[2] - pos1[0]\n",
        "\n",
        "    dify = 0  # Die Überlappung in y-Richtung.\n",
        "    if pos2[1] < pos1[1] < pos2[3]:\n",
        "        dify = pos2[3] - pos1[1]\n",
        "        if pos2[1] < pos1[3] < pos2[3]:\n",
        "            dify = pos2[3] - pos2[1]\n",
        "\n",
        "    elif pos1[1] < pos2[1] < pos1[3]:\n",
        "        dify = pos1[3] - pos2[1]\n",
        "        if pos1[1] < pos2[3] < pos1[3]:\n",
        "            dify = pos1[3] - pos1[1]\n",
        "\n",
        "    overlap = difx * dify  # Die Fläche der Überlappung.\n",
        "\n",
        "    A_pos1 = (pos1[2] - pos1[0]) * (pos1[3] - pos1[1])  # Die Fläche von Box 1.\n",
        "    A_pos2 = (pos2[2] - pos2[0]) * (pos2[3] - pos2[1])  # Die Fläche von Box 2.\n",
        "    # Das Verhältniss der Fläche der Überlappung und der Fläche der kleineren Box wird in Prozent zurückgegeben.\n",
        "    return (overlap / A_pos1, 1) if A_pos1 < A_pos2 else (overlap / A_pos2, 2)\n",
        "\n",
        "\n",
        "def decimal_coords(coords, ref):\n",
        "    \"\"\"\n",
        "    Diese Funktion konvertiert die Koorinaten coords (mit der Referenz ref) in Dezimalkoordinaten.\n",
        "    \"\"\"\n",
        "    decimal_degrees = coords[0] + coords[1] / 60 + coords[2] / 3600\n",
        "    if ref == \"S\" or ref == \"W\":\n",
        "        decimal_degrees = -decimal_degrees\n",
        "    return decimal_degrees\n",
        "\n",
        "\n",
        "def get_flight_yaw_altitude_coords(image_path):\n",
        "    \"\"\"\n",
        "    Diese Funktion liest die Höhe, die Koordinaten und den Winkel nach Norden der Drohen aus den Metadaten des Bildes\n",
        "    an der Position image_path aus.\n",
        "    \"\"\"\n",
        "    with open(image_path, \"rb\") as img:\n",
        "        img_text = img.read()\n",
        "        image = EImage(img_text)\n",
        "        coords = (decimal_coords(image.gps_latitude,\n",
        "                                 image.gps_latitude_ref),\n",
        "                  decimal_coords(image.gps_longitude,\n",
        "                                 image.gps_longitude_ref))\n",
        "        # Die Koordinaten können aus den exif-Metadaten der Bilder ausgelesen werden.\n",
        "        start_pos = img_text.find(b\"FlightYawDegree=\") + 17  # Der Winkel nach Norden und die Höhe der Drohne können\n",
        "        # einfach aus den Bytes des Bildes ausgelesen werden.\n",
        "        for i, b in enumerate(img_text[start_pos:]):\n",
        "            if b == 34:\n",
        "                end_pos = i\n",
        "                break\n",
        "        yaw = float(img_text[start_pos:end_pos + start_pos])\n",
        "        start_pos = img_text.find(b\"RelativeAltitude=\") + 18\n",
        "        for i, b in enumerate(img_text[start_pos:]):\n",
        "            if b == 34:\n",
        "                end_pos = i\n",
        "                break\n",
        "        alt = float(img_text[start_pos:end_pos + start_pos])\n",
        "        return yaw, alt, coords\n",
        "\n",
        "\n",
        "def get_Angle_from_Camera(x: int, y: int):\n",
        "    \"\"\"\n",
        "    Diese Funktion gibt die Distanz des Bildpunktes x y zum Bildmittelpunkt und den Winkel zur y-Achse zurück.\n",
        "    \"\"\"\n",
        "    new_x = x - 2000\n",
        "    new_y = y - 1500\n",
        "    dist = 0\n",
        "    if new_x != 0 and new_y != 0:\n",
        "        dist = math.sqrt(pow(new_x, 2) + pow(new_y, 2))\n",
        "    return dist, math.atan2(y - 1500, x - 2000) + math.pi / 2\n",
        "\n",
        "\n",
        "def get_Ampfer_Cords(filename, boxes, Img_dir):\n",
        "    \"\"\"\n",
        "    Diese Funktion gibt eine Liste an Koordinaten zurück, an denen die Ampfer sind, deren Position auf dem Bild in der\n",
        "    boxes Liste gespeichert ist.\n",
        "    \"\"\"\n",
        "    IMG_PATH = f\"{Img_dir}{filename}.JPG\"\n",
        "    ampfer_coords = []\n",
        "    yaw, alt, coords = get_flight_yaw_altitude_coords(IMG_PATH)  # Die Metadaten werden ausgelesen.\n",
        "    meter_per_pixel = (alt * math.tan(math.radians(39.5))) / 2500  # Ein Umrechnugsfaktor, wie viele Pixel im Bild\n",
        "    # einem Meter entsprechen, wird berechnet.\n",
        "    for x, y in boxes:\n",
        "        dist_pixel, angle_from_middle = get_Angle_from_Camera(x, y)\n",
        "        abs_angle = math.radians(yaw) + angle_from_middle  # Die Distanz zum Bildmittelpunkt und der Winkel nach\n",
        "        dist = dist_pixel * meter_per_pixel  # Norden wird für jeden Ampfer berechnet.\n",
        "        ampfer_coords.append(inverse_haversine(coords, dist, abs_angle, unit=haversine.Unit.METERS))\n",
        "        # Dann werden die Koordinaten des Ampfers berechnet und an die amper_coords-Liste angehängt.\n",
        "    return ampfer_coords\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = Dataloader(transforms=T.Compose([T.ToTensor()]))\n",
        "    torch.manual_seed(1)\n",
        "    indices = torch.randperm(len(dataset)).tolist()\n",
        "    dataset = torch.utils.data.Subset(dataset, indices)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        Dataloader(transforms=T.Compose(T.ToTensor())), batch_size=2, shuffle=False, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "    loaded_model = get_model(num_classes=2).eval()\n",
        "    # Zuerst wird das Netz und der Dataloader initialisiert.\n",
        "\n",
        "    for idx in range(len(os.listdir(bilderp))):\n",
        "\n",
        "        img, filename = dataset[idx]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = loaded_model([img])  # Das Netz wird dann auf jedes Bild angewendet.\n",
        "        objects = []\n",
        "        pic_name = filename[\"filename\"]\n",
        "        for element in range(len(prediction[0][\"boxes\"])):\n",
        "            boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
        "            pos = (int(boxes[0]), int(boxes[1]), int(boxes[2]), int(boxes[3]))\n",
        "            objects.append(pos)\n",
        "        # Dann wird die boxes-Liste anhand der Netzausgabe erstellt.\n",
        "        img = Img(pic_name[:-4], objects)\n",
        "        img.remove_overlap()  # Mit der remove_overlap-Funtkion der Image-Klasse werden dann die Überlappungen entfernt.\n",
        "        boxes = []\n",
        "        for box in img.objects:\n",
        "            boxes.append(((box[0] + box[2]) / 2, (box[1] + box[3]) / 2))\n",
        "        cordslist = get_Ampfer_Cords(pic_name[:-4], boxes, bilderp)  # Anhand der Bildkoordinaten des Ampfers werden\n",
        "        # dann die realen Koordinaten des Ampfers berechet.\n",
        "        json.dumps(cordslist)\n",
        "        with open(targetp + pic_name[:-4] + \".json\", \"w\") as f:\n",
        "            f.write(json.dumps(cordslist))\n",
        "        kml = simplekml.Kml()\n",
        "        for i in cordslist:\n",
        "            kml.newpoint(name=\"Ampfer\", coords=[(i[1], i[0])])\n",
        "        kml.save(f\"{targetp}/{pic_name[:-4]}.kml\")\n",
        "        # Diese Koordinaten werden dann in einer .json und in einer .kml Datei gespeichert.\n",
        "        # .kml Dateien können in Google maps geöffnet werden\n",
        "        print(pic_name + \" done!\")"
      ],
      "metadata": {
        "id": "uhbceeKCRPyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Einzeichnen der Ampferpositionen auf den Bildern (Drohnenbilder)\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import PIL\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import time\n",
        "\n",
        "bilderp = \"Bilder/\"\n",
        "if not os.path.exists(\"Output\"):\n",
        "  os.mkdir(\"Output\")\n",
        "outputp = \"Output/\"\n",
        "\n",
        "\n",
        "class Dataloader(torch.utils.data.Dataset):\n",
        "    def __init__(self, transforms=None):\n",
        "        self.Bilder = bilderp\n",
        "        self.pdir = sorted(os.listdir(self.Bilder))\n",
        "        print(self.pdir)\n",
        "        self.Data = {}\n",
        "        self.Boxes = []\n",
        "        self.Transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):  # returns a dictionary with filenames as keys\n",
        "        # only returns the labeled pictures as a list: [Boxes,RGB-Image]\n",
        "\n",
        "        img = Image.open(self.Bilder + self.pdir[idx]).convert(\"RGB\")\n",
        "        print(self.pdir[idx])\n",
        "\n",
        "        target = {\"filename\":self.pdir[idx]}\n",
        "        if self.Transforms is not None:\n",
        "            img, target = self.Transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.pdir))\n",
        "\n",
        "\n",
        "def get_model(num_classes):\n",
        "    # load an object detection model pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new on\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    if os.path.isfile(\"Networks/model_drohne_niedrig_V2.pt\"):\n",
        "        print(\"---loaded model---\")\n",
        "        model.load_state_dict(torch.load(\"Networks/model_drohne_niedrig_V2.pt\"))\n",
        "    else:\n",
        "        logging.warning(\"model not found\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "\n",
        "dataset = Dataloader(transforms=get_transform(train=False))\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices)\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    Dataloader(transforms=get_transform(train=False)), batch_size=2, shuffle=False, num_workers=2,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "loaded_model = get_model(num_classes=2)\n",
        "\n",
        "for idx in range(len(os.listdir(bilderp))):\n",
        "\n",
        "  img, target = dataset[idx]\n",
        "  \n",
        "  #put the model in evaluation mode\n",
        "  loaded_model.eval()\n",
        "  with torch.no_grad():\n",
        "    prediction = loaded_model([img])\n",
        "\n",
        "  image = Image.fromarray(img.mul(255).permute(1, 2,0).byte().numpy())\n",
        "  draw = ImageDraw.Draw(image)\n",
        "\n",
        "\n",
        "  for element in range(len(prediction[0][\"boxes\"])):\n",
        "    boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
        "    score = np.round(prediction[0][\"scores\"][element].cpu().numpy(),\n",
        "                      decimals= 4)\n",
        "    if True:\n",
        "        draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])], \n",
        "        outline =\"red\", width =10)\n",
        "        fnt = ImageFont.truetype(\"usr/share/fronts/truetype/liberation/LiberationMono-Regular.ttf\",  size=100)\n",
        "        draw.text((boxes[0]+10, boxes[1]+10), text = str(score), fill=(255,255,255,255))\n",
        "  image.save(outputp + str(target[\"filename\"])[:-4] + \".JPG\")\n",
        "  display(image)"
      ],
      "metadata": {
        "id": "i_DRaxSPs4rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Einzeichen der Ampferpositionen auf Bildern aus 1m Höhe\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "from PIL import ImageDraw,Image\n",
        "\n",
        "\n",
        "bilderp = \"Bilder_1m/\"\n",
        "\n",
        "class Dataloader(torch.utils.data.Dataset):\n",
        "  def __init__(self,transforms=None):\n",
        "    self.Bilder = bilderp\n",
        "    self.pdir = sorted(os.listdir(self.Bilder))\n",
        "\n",
        "    self.Data = {}\n",
        "    self.Boxes = []\n",
        "    self.Transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):  #returns a dictionary with filenames as keys\n",
        "    # only returns the labeled pictures as a list: [Boxes,RGB-Image]\n",
        "    \n",
        "    img = Image.open(self.Bilder + self.pdir[idx]).convert(\"RGB\")\n",
        "    target = {}\n",
        "    if self.Transforms is not None:\n",
        "      img, target = self.Transforms(img, target)\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(len(self.pdir))\n",
        "\n",
        "\n",
        "   \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "def get_model(num_classes):\n",
        "  # load an object detection model pre-trained on COCO\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  # get the number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # replace the pre-trained head with a new on\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n",
        "    \n",
        "  if os.path.isfile(\"Networks/model_jungV2.pt\"):\n",
        "    print(\"---loaded model---\")\n",
        "    model.load_state_dict(torch.load(\"Networks/model_jungV2.pt\"))\n",
        "  return model\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "      # during training, randomly flip the training images\n",
        "      transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "dataset = Dataloader(transforms=get_transform(train=True))\n",
        "dataset_test = Dataloader(transforms = get_transform(train=False))\n",
        "               \n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-30])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-30:])\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "              Dataloader(transforms=get_transform(train=True)), batch_size=2, shuffle=True, num_workers=2,\n",
        "              collate_fn=utils.collate_fn)\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "         Dataloader(transforms = get_transform(train=False)), batch_size=1, shuffle=False, num_workers=2,\n",
        "         collate_fn=utils.collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "loaded_model = get_model(num_classes = 2)\n",
        "\n",
        "loaded_model.load_state_dict(torch.load(\"Networks/model_jungV2.pt\"))\n",
        "\n",
        "for idx in range(len(os.listdir(bilderp))):\n",
        "\n",
        "  img, _ = dataset_test[idx]\n",
        "\n",
        "  #put the model in evaluation mode\n",
        "  loaded_model.eval()\n",
        "  with torch.no_grad():\n",
        "    prediction = loaded_model([img])\n",
        "  image = Image.fromarray(img.mul(255).permute(1, 2,0).byte().numpy())\n",
        "  draw = ImageDraw.Draw(image)\n",
        "\n",
        "\n",
        "  for element in range(len(prediction[0][\"boxes\"])):\n",
        "    boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
        "    score = np.round(prediction[0][\"scores\"][element].cpu().numpy(),\n",
        "                      decimals= 4)\n",
        "    if score > 0.8:\n",
        "        draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])], \n",
        "        outline =\"red\", width =10)\n",
        "        draw.rectangle([(boxes[0] + 10  + (boxes[2]-boxes[0])/2,boxes[1] + 10 + (boxes[3]-boxes[1])/2),(boxes[0] - 10  + (boxes[2]-boxes[0])/2,boxes[1] - 10 + (boxes[3]-boxes[1])/2)],outline=\"blue\",width=20)\n",
        "        draw.text((boxes[0], boxes[1]), text = str(score),width=30,outline=\"blue\")\n",
        "\n",
        "  image.save(\"Output/\" + str(idx) + \".JPG\")\n",
        "  display(image)\n"
      ],
      "metadata": {
        "id": "6Ae4hh2Vurqa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}