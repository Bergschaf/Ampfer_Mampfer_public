{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOidPYodHsqtk7PhqujDfU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bergschaf/Ampfer_Mampfer_public/blob/main/Ampfer_Mampfer_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOahqX1nQwJK",
        "outputId": "a5fa1656-222e-45e7-a55e-0eef3801f836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting haversine\n",
            "  Downloading haversine-2.7.0-py2.py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: haversine\n",
            "Successfully installed haversine-2.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simplekml\n",
            "  Downloading simplekml-1.3.6.tar.gz (52 kB)\n",
            "Building wheels for collected packages: simplekml\n",
            "  Building wheel for simplekml (setup.py): started\n",
            "  Building wheel for simplekml (setup.py): finished with status 'done'\n",
            "  Created wheel for simplekml: filename=simplekml-1.3.6-py3-none-any.whl size=65876 sha256=2f1145163dac087dc7f632c99d275e461414ca5ceee88265a7434f63aea365b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/ec/e6/10af1a1fb29ffca95151d4c886d6e06fc309c68f46519892de\n",
            "Successfully built simplekml\n",
            "Installing collected packages: simplekml\n",
            "Successfully installed simplekml-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting exif\n",
            "  Downloading exif-1.3.5-py3-none-any.whl (29 kB)\n",
            "Collecting plum-py<2.0.0,>=0.5.0\n",
            "  Downloading plum_py-0.8.1-py3-none-any.whl (93 kB)\n",
            "Installing collected packages: plum-py, exif\n",
            "Successfully installed exif-1.3.5 plum-py-0.8.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'vision'...\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be376084d version check against PyTorch's CUDA version\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "%%bash\n",
        "pip install haversine\n",
        "pip install simplekml\n",
        "pip install exif\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../\n",
        "cd ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Images\n",
        "import os\n",
        "if not os.path.exists(\"Bilder\"):\n",
        "  os.mkdir(\"Bilder\") \n",
        "!gdown --folder https://drive.google.com/drive/folders/1U898P7M925KQ89KvOIjYJAwuzZxjRDtP?usp=sharing -q\n",
        "os.chdir(\"/content\")"
      ],
      "metadata": {
        "id": "s-xFxx14mjZo",
        "outputId": "b8efca45-2015-4455-dcae-f7f48ee2f958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/adapters.py\", line 449, in send\n",
            "    timeout=timeout\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
            "    chunked=chunked)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 343, in _make_request\n",
            "    self._validate_conn(conn)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 839, in _validate_conn\n",
            "    conn.connect()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\", line 332, in connect\n",
            "    cert_reqs=resolve_cert_reqs(self.cert_reqs),\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/util/ssl_.py\", line 272, in create_urllib3_context\n",
            "    options |= OP_NO_SSLv2\n",
            "  File \"/usr/lib/python3.7/enum.py\", line 838, in __or__\n",
            "    result = self.__class__(self._value_ | self.__class__(other)._value_)\n",
            "  File \"/usr/lib/python3.7/enum.py\", line 315, in __call__\n",
            "    return cls.__new__(cls, value)\n",
            "  File \"/usr/lib/python3.7/enum.py\", line 541, in __new__\n",
            "    return cls._value2member_map_[value]\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gdown/cli.py\", line 152, in main\n",
            "    remaining_ok=args.remaining_ok,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gdown/download_folder.py\", line 349, in download_folder\n",
            "    use_cookies=use_cookies,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gdown/download.py\", line 146, in download\n",
            "    res = sess.get(url, headers=headers, stream=True, verify=verify)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 543, in get\n",
            "    return self.request('GET', url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 530, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 665, in send\n",
            "    history = [resp for resp in gen] if allow_redirects else []\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 665, in <listcomp>\n",
            "    history = [resp for resp in gen] if allow_redirects else []\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 245, in resolve_redirects\n",
            "    **adapter_kwargs\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 643, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/adapters.py\", line 497, in send\n",
            "    except (ProtocolError, socket.error) as err:\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Networks\n",
        "import os\n",
        "if not os.path.exists(\"Networks\"):\n",
        "  os.mkdir(\"Networks\") \n",
        "os.chdir(\"Networks\")\n",
        "!gdown 1k3Yjk6Emb_ePUgzQIfSdJq8jxgRoNpg4\n",
        "os.chdir(\"/content\")"
      ],
      "metadata": {
        "id": "PeE6Vk99n8ZE",
        "outputId": "2032c3d5-3976-4abc-e60e-8267bee42332",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1k3Yjk6Emb_ePUgzQIfSdJq8jxgRoNpg4\n",
            "To: /content/Networks/model_drohne_niedrig_V2.pt\n",
            "100% 166M/166M [00:00<00:00, 266MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ermitteln der Koordinaten der Ampfer\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from engine import evaluate\n",
        "from PIL import Image\n",
        "import utils\n",
        "import transforms as T\n",
        "import os\n",
        "import math\n",
        "import haversine\n",
        "from haversine import inverse_haversine\n",
        "from exif import Image as EImage\n",
        "import json\n",
        "import simplekml\n",
        "\n",
        "\n",
        "if not os.path.exists(\"Koordinaten\"):\n",
        "  os.mkdir(\"Koordinaten\")\n",
        "bilderp = \"Bilder/\"  # Ordner, in den die Bilder hochgeladen wurden\n",
        "targetp = \"Koordinaten/\"  # Ordner, in den die ermittelten Koordinaten gespeichert werden\n",
        "\n",
        "\n",
        "class Dataloader(torch.utils.data.Dataset):\n",
        "    def __init__(self, transforms=None):\n",
        "        self.Bilder = bilderp\n",
        "        self.pdir = sorted(os.listdir(self.Bilder))\n",
        "        self.Data = {}\n",
        "        self.Boxes = []\n",
        "        self.Transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Zum Trainieren des Netzes soll diese Funktion, die Label des Bildes an der Position idx\n",
        "        zurückgeben. Da hier das Netz nur angewendet und nicht trainiert wird, werden diese Label\n",
        "        nicht benötigt. Anstelle der Label wird daher der Dateiname des Bildes zurückgegeben, sodass\n",
        "        man die erkannten Ampfer später den Metadaten des Bildes zuordnen kann.\n",
        "        \"\"\"\n",
        "        img = Image.open(self.Bilder + self.pdir[idx]).convert(\"RGB\")\n",
        "        filename = {\"filename\": self.pdir[idx]}\n",
        "        if self.Transforms is not None:\n",
        "            img, filename = self.Transforms(img, filename)\n",
        "        return img, filename\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.pdir))\n",
        "\n",
        "\n",
        "def get_model(num_classes):\n",
        "    \"\"\"\n",
        "    Diese Funktion ist dafür zuständig, das Netz zu laden.\n",
        "    \"\"\"\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)  # Das pretrained-Model von pytorch\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features  # wird heruntergeladen und konfiguriert.\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    \n",
        "    if os.path.isfile(\"Networks/model_drohne_niedrig_V2.pt\"):\n",
        "        print(\"---loaded model---\")\n",
        "        model.load_state_dict(torch.load(\"Networks/model_drohne_niedrig_V2.pt\"))\n",
        "    else:\n",
        "        print(\"---model not found---\")\n",
        "    # Wenn bereits ein trainiertes Netz vorhanden ist, dann werden die entsprechenden Gewichtungen geladen:\n",
        "    return model\n",
        "\n",
        "\n",
        "class Img:\n",
        "    \"\"\"\n",
        "    Die Img Klasse wird für Bilder erstellt, um die Überlappung der Boxen, die die Ampferpflanzen markieren, zu\n",
        "    entfernen.\n",
        "    \"\"\"\n",
        "    def __init__(self, filename: str, objects):\n",
        "        self.filename = filename  # Dateiname des Bildes (ohne Dateiendung)\n",
        "        self.objects = objects  # Die erkannten Positionen der Ampferpflanzen.\n",
        "\n",
        "    def remove_overlap(self):\n",
        "        \"\"\"\n",
        "        Diese Funktion überprüft, ob sich die Boxen überlappen.\n",
        "        \"\"\"\n",
        "        for index, object in enumerate(self.objects):\n",
        "            for index2, object2 in enumerate(self.objects[index:]):\n",
        "                # Das Programm überorüft für jede Kombination von zwei Boxen, ob sie sich überlappen.\n",
        "                overlap, smaller = check_overlap(object, object2)  # Die check_overlap-Funktion gibt zurück, zu wie groß\n",
        "                # die sich überlappende Fläche im Verlgeich zur Fläche der kleineren Box ist (in Prozent).\n",
        "                if overlap > 0.8:  # Wenn sich die Boxen zu mehr als 80 % überlappen, dann wird die kleinere gelöscht.\n",
        "                    try:\n",
        "                        if smaller == 1:  # Die smaller Variable gibt an, welche Box kleiner ist.\n",
        "                            self.objects.remove(object)\n",
        "                        else:\n",
        "                            self.objects.remove(object2)\n",
        "                    except ValueError:  # Wenn die Box schon gelöscht wurde, dann wird nichts gemacht.\n",
        "                        pass\n",
        "\n",
        "\n",
        "def check_overlap(pos1, pos2):\n",
        "    \"\"\"\n",
        "    Die check_overlap-Funktion gibt zurück, wie viel sich die zwei Boxen pos1 und pos2 überlappen.\n",
        "    \"\"\"\n",
        "    difx = 0  # Die überlappung in x-Richtung.\n",
        "    if pos2[0] < pos1[0] < pos2[2]:\n",
        "        difx = pos2[2] - pos1[0]\n",
        "        if pos2[0] < pos1[2] < pos2[2]:\n",
        "            difx = pos2[2] - pos2[0]\n",
        "\n",
        "    elif pos1[0] < pos2[0] < pos1[2]:\n",
        "        difx = pos1[2] - pos2[0]\n",
        "        if pos1[0] < pos2[2] < pos1[2]:\n",
        "            difx = pos1[2] - pos1[0]\n",
        "\n",
        "    dify = 0  # Die Überlappung in y-Richtung.\n",
        "    if pos2[1] < pos1[1] < pos2[3]:\n",
        "        dify = pos2[3] - pos1[1]\n",
        "        if pos2[1] < pos1[3] < pos2[3]:\n",
        "            dify = pos2[3] - pos2[1]\n",
        "\n",
        "    elif pos1[1] < pos2[1] < pos1[3]:\n",
        "        dify = pos1[3] - pos2[1]\n",
        "        if pos1[1] < pos2[3] < pos1[3]:\n",
        "            dify = pos1[3] - pos1[1]\n",
        "\n",
        "    overlap = difx * dify  # Die Fläche der Überlappung.\n",
        "\n",
        "    A_pos1 = (pos1[2] - pos1[0]) * (pos1[3] - pos1[1])  # Die Fläche von Box 1.\n",
        "    A_pos2 = (pos2[2] - pos2[0]) * (pos2[3] - pos2[1])  # Die Fläche von Box 2.\n",
        "    # Das Verhältniss der Fläche der Überlappung und der Fläche der kleineren Box wird in Prozent zurückgegeben.\n",
        "    return (overlap / A_pos1, 1) if A_pos1 < A_pos2 else (overlap / A_pos2, 2)\n",
        "\n",
        "\n",
        "def decimal_coords(coords, ref):\n",
        "    \"\"\"\n",
        "    Diese Funktion konvertiert die Koorinaten coords (mit der Referenz ref) in Dezimalkoordinaten.\n",
        "    \"\"\"\n",
        "    decimal_degrees = coords[0] + coords[1] / 60 + coords[2] / 3600\n",
        "    if ref == \"S\" or ref == \"W\":\n",
        "        decimal_degrees = -decimal_degrees\n",
        "    return decimal_degrees\n",
        "\n",
        "\n",
        "def get_flight_yaw_altitude_coords(image_path):\n",
        "    \"\"\"\n",
        "    Diese Funktion liest die Höhe, die Koordinaten und den Winkel nach Norden der Drohen aus den Metadaten des Bildes\n",
        "    an der Position image_path aus.\n",
        "    \"\"\"\n",
        "    with open(image_path, \"rb\") as img:\n",
        "        img_text = img.read()\n",
        "        image = EImage(img_text)\n",
        "        coords = (decimal_coords(image.gps_latitude,\n",
        "                                 image.gps_latitude_ref),\n",
        "                  decimal_coords(image.gps_longitude,\n",
        "                                 image.gps_longitude_ref))\n",
        "        # Die Koordinaten können aus den exif-Metadaten der Bilder ausgelesen werden.\n",
        "        start_pos = img_text.find(b\"FlightYawDegree=\") + 17  # Der Winkel nach Norden und die Höhe der Drohne können\n",
        "        # einfach aus den Bytes des Bildes ausgelesen werden.\n",
        "        for i, b in enumerate(img_text[start_pos:]):\n",
        "            if b == 34:\n",
        "                end_pos = i\n",
        "                break\n",
        "        yaw = float(img_text[start_pos:end_pos + start_pos])\n",
        "        start_pos = img_text.find(b\"RelativeAltitude=\") + 18\n",
        "        for i, b in enumerate(img_text[start_pos:]):\n",
        "            if b == 34:\n",
        "                end_pos = i\n",
        "                break\n",
        "        alt = float(img_text[start_pos:end_pos + start_pos])\n",
        "        return yaw, alt, coords\n",
        "\n",
        "\n",
        "def get_Angle_from_Camera(x: int, y: int):\n",
        "    \"\"\"\n",
        "    Diese Funktion gibt die Distanz des Bildpunktes x y zum Bildmittelpunkt und den Winkel zur y-Achse zurück.\n",
        "    \"\"\"\n",
        "    new_x = x - 2000\n",
        "    new_y = y - 1500\n",
        "    dist = 0\n",
        "    if new_x != 0 and new_y != 0:\n",
        "        dist = math.sqrt(pow(new_x, 2) + pow(new_y, 2))\n",
        "    return dist, math.atan2(y - 1500, x - 2000) + math.pi / 2\n",
        "\n",
        "\n",
        "def get_Ampfer_Cords(filename, boxes, Img_dir):\n",
        "    \"\"\"\n",
        "    Diese Funktion gibt eine Liste an Koordinaten zurück, an denen die Ampfer sind, deren Position auf dem Bild in der\n",
        "    boxes Liste gespeichert ist.\n",
        "    \"\"\"\n",
        "    IMG_PATH = f\"{Img_dir}{filename}.JPG\"\n",
        "    ampfer_coords = []\n",
        "    yaw, alt, coords = get_flight_yaw_altitude_coords(IMG_PATH)  # Die Metadaten werden ausgelesen.\n",
        "    meter_per_pixel = (alt * math.tan(math.radians(39.5))) / 2500  # Ein Umrechnugsfaktor, wie viele Pixel im Bild\n",
        "    # einem Meter entsprechen, wird berechnet.\n",
        "    for x, y in boxes:\n",
        "        dist_pixel, angle_from_middle = get_Angle_from_Camera(x, y)\n",
        "        abs_angle = math.radians(yaw) + angle_from_middle  # Die Distanz zum Bildmittelpunkt und der Winkel nach\n",
        "        dist = dist_pixel * meter_per_pixel  # Norden wird für jeden Ampfer berechnet.\n",
        "        ampfer_coords.append(inverse_haversine(coords, dist, abs_angle, unit=haversine.Unit.METERS))\n",
        "        # Dann werden die Koordinaten des Ampfers berechnet und an die amper_coords-Liste angehängt.\n",
        "    return ampfer_coords\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = Dataloader(transforms=T.Compose([T.ToTensor()]))\n",
        "    torch.manual_seed(1)\n",
        "    indices = torch.randperm(len(dataset)).tolist()\n",
        "    dataset = torch.utils.data.Subset(dataset, indices)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        Dataloader(transforms=T.Compose(T.ToTensor())), batch_size=2, shuffle=False, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "    loaded_model = get_model(num_classes=2).eval()\n",
        "    # Zuerst wird das Netz und der Dataloader initialisiert.\n",
        "\n",
        "    for idx in range(len(os.listdir(bilderp))):\n",
        "\n",
        "        img, filename = dataset[idx]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = loaded_model([img])  # Das Netz wird dann auf jedes Bild angewendet.\n",
        "        objects = []\n",
        "        pic_name = filename[\"filename\"]\n",
        "        for element in range(len(prediction[0][\"boxes\"])):\n",
        "            boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
        "            pos = (int(boxes[0]), int(boxes[1]), int(boxes[2]), int(boxes[3]))\n",
        "            objects.append(pos)\n",
        "        # Dann wird die boxes-Liste anhand der Netzausgabe erstellt.\n",
        "        img = Img(pic_name[:-4], objects)\n",
        "        img.remove_overlap()  # Mit der remove_overlap-Funtkion der Image-Klasse werden dann die Überlappungen entfernt.\n",
        "        boxes = []\n",
        "        for box in img.objects:\n",
        "            boxes.append(((box[0] + box[2]) / 2, (box[1] + box[3]) / 2))\n",
        "        cordslist = get_Ampfer_Cords(pic_name[:-4], boxes, bilderp)  # Anhand der Bildkoordinaten des Ampfers werden\n",
        "        # dann die realen Koordinaten des Ampfers berechet.\n",
        "        json.dumps(cordslist)\n",
        "        with open(targetp + pic_name[:-4] + \".json\", \"w\") as f:\n",
        "            f.write(json.dumps(cordslist))\n",
        "        kml = simplekml.Kml()\n",
        "        for i in cordslist:\n",
        "            kml.newpoint(name=\"Ampfer\", coords=[(i[1], i[0])])\n",
        "        kml.save(f\"{targetp}/{pic_name[:-4]}.kml\")\n",
        "        # Diese Koordinaten werden dann in einer .json und in einer .kml Datei gespeichert.\n",
        "        # .kml Dateien können in Google maps geöffnet werden\n",
        "        print(pic_name + \" done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "uhbceeKCRPyF",
        "outputId": "c50be1af-9cf9-4692-b629-713d2edae8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---loaded model---\n",
            "131.JPG done!\n",
            "139.JPG done!\n",
            "128.JPG done!\n",
            "137.JPG done!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-1bb9b67f4d6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbilderp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-1bb9b67f4d6b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mman\u001b[0m \u001b[0mdie\u001b[0m \u001b[0merkannten\u001b[0m \u001b[0mAmpfer\u001b[0m \u001b[0mspäter\u001b[0m \u001b[0mden\u001b[0m \u001b[0mMetadaten\u001b[0m \u001b[0mdes\u001b[0m \u001b[0mBildes\u001b[0m \u001b[0mzuordnen\u001b[0m \u001b[0mkann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBilder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'Bilder/Bilder'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Einzeichnen der Ampferpositionen auf den Bildern\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import PIL\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import time\n",
        "\n",
        "bilderp = \"Bilder/\"\n",
        "if not os.path.exists(\"Output\"):\n",
        "  os.mkdir(\"Output\")\n",
        "outputp = \"Output/\"\n",
        "\n",
        "\n",
        "class Dataloader(torch.utils.data.Dataset):\n",
        "    def __init__(self, transforms=None):\n",
        "        self.Bilder = bilderp\n",
        "        self.pdir = sorted(os.listdir(self.Bilder))\n",
        "        print(self.pdir)\n",
        "        self.Data = {}\n",
        "        self.Boxes = []\n",
        "        self.Transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):  # returns a dictionary with filenames as keys\n",
        "        # only returns the labeled pictures as a list: [Boxes,RGB-Image]\n",
        "\n",
        "        img = Image.open(self.Bilder + self.pdir[idx]).convert(\"RGB\")\n",
        "        print(self.pdir[idx])\n",
        "\n",
        "        target = {\"filename\":self.pdir[idx]}\n",
        "        if self.Transforms is not None:\n",
        "            img, target = self.Transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.pdir))\n",
        "\n",
        "\n",
        "def get_model(num_classes):\n",
        "    # load an object detection model pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new on\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    if os.path.isfile(\"Networks/model_drohne_niedrig_V2.pt\"):\n",
        "        print(\"---loaded model---\")\n",
        "        model.load_state_dict(torch.load(\"Networks/model_drohne_niedrig_V2.pt\"))\n",
        "    else:\n",
        "        logging.warning(\"model not found\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "\n",
        "dataset = Dataloader(transforms=get_transform(train=False))\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices)\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    Dataloader(transforms=get_transform(train=False)), batch_size=2, shuffle=False, num_workers=2,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "loaded_model = get_model(num_classes=2)\n",
        "\n",
        "for idx in range(len(os.listdir(bilderp))):\n",
        "\n",
        "  img, target = dataset[idx]\n",
        "  \n",
        "  #put the model in evaluation mode\n",
        "  loaded_model.eval()\n",
        "  with torch.no_grad():\n",
        "    prediction = loaded_model([img])\n",
        "\n",
        "  image = Image.fromarray(img.mul(255).permute(1, 2,0).byte().numpy())\n",
        "  draw = ImageDraw.Draw(image)\n",
        "\n",
        "\n",
        "  for element in range(len(prediction[0][\"boxes\"])):\n",
        "    boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
        "    score = np.round(prediction[0][\"scores\"][element].cpu().numpy(),\n",
        "                      decimals= 4)\n",
        "    if True:\n",
        "        draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])], \n",
        "        outline =\"red\", width =10)\n",
        "        fnt = ImageFont.truetype(\"usr/share/fronts/truetype/liberation/LiberationMono-Regular.ttf\",  size=100)\n",
        "        draw.text((boxes[0]+10, boxes[1]+10), text = str(score), fill=(255,255,255,255))\n",
        "  image.save(outputp + str(target[\"filename\"])[:-4] + \".JPG\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "i_DRaxSPs4rD",
        "outputId": "8a60f729-e109-467d-9201-25b1ed505ead"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['126.JPG', '127.JPG', '128.JPG', '129.JPG', '130.JPG', '131.JPG', '132.JPG', '133.JPG', '134.JPG', '97.JPG', '98.JPG', '99.JPG', 'Bilder']\n",
            "['126.JPG', '127.JPG', '128.JPG', '129.JPG', '130.JPG', '131.JPG', '132.JPG', '133.JPG', '134.JPG', '97.JPG', '98.JPG', '99.JPG', 'Bilder']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---loaded model---\n",
            "132.JPG\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9a40b45ed913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbilderp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;31m#put the model in evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-9a40b45ed913>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# only returns the labeled pictures as a list: [Boxes,RGB-Image]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBilder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'Bilder/Bilder'"
          ]
        }
      ]
    }
  ]
}